# ü§ñ Ollama Setup - Schritt f√ºr Schritt

## Problem: "Keine Verbindung zu Ollama"

Wenn Sie diese Meldung sehen, bedeutet das: **Ollama l√§uft nicht!**

---

## ‚úÖ L√∂sung in 3 Schritten:

### Schritt 1: Ollama starten

√ñffnen Sie ein Terminal und geben Sie ein:

```bash
ollama serve
```

**Lassen Sie dieses Terminal-Fenster OFFEN!** Ollama l√§uft jetzt im Hintergrund.

Sie sollten sehen:
```
time=... level=INFO source=... msg="Listening on 127.0.0.1:11434..."
```

---

### Schritt 2: Model herunterladen (falls noch nicht geschehen)

√ñffnen Sie ein **ZWEITES** Terminal und geben Sie ein:

```bash
# Empfohlen: Llama 3 (4GB)
ollama pull llama3

# ODER andere Models:
ollama pull mistral      # 4GB
ollama pull phi          # 1.5GB (klein & schnell)
ollama pull codellama    # 7GB (f√ºr Code)
```

Pr√ºfen Sie installierte Models:
```bash
ollama list
```

---

### Schritt 3: In App-Settings konfigurieren

1. √ñffnen Sie die Trading-App
2. Klicken Sie auf **‚öôÔ∏è Einstellungen**
3. Bei **"KI Provider"** w√§hlen Sie: **Ollama (Lokal)**
4. Bei **"Ollama Model"** w√§hlen Sie Ihr installiertes Model (z.B. `llama3`)
5. **"Einstellungen speichern"** klicken

---

## ‚úÖ Testen

1. Gehen Sie zum **Chat-Tab**
2. Schreiben Sie eine Nachricht, z.B.: "Wie sieht Gold aus?"
3. Die KI sollte jetzt antworten! üéâ

---

## üîß Troubleshooting

### Problem: "Model nicht gefunden"

**L√∂sung:**
```bash
ollama pull llama3
```

Dann in App-Settings den richtigen Model-Namen eingeben (genau wie bei `ollama list` angezeigt).

---

### Problem: "Connection refused"

**M√∂gliche Ursachen:**

1. **Ollama l√§uft nicht**
   ```bash
   # Starten Sie Ollama:
   ollama serve
   ```

2. **Falscher Port**
   - Standard ist: `http://localhost:11434`
   - Pr√ºfen Sie in App-Settings unter "Ollama Server URL"

3. **Ollama l√§uft auf anderem Port**
   ```bash
   # Pr√ºfen Sie, auf welchem Port Ollama l√§uft:
   lsof -i :11434
   ```

---

### Problem: "Ollama antwortet sehr langsam"

**Ursachen:**
- Ihr Mac braucht mehr RAM/CPU
- Das Model ist zu gro√ü

**L√∂sung:**
```bash
# Verwenden Sie ein kleineres Model:
ollama pull phi  # Nur 1.5GB, sehr schnell!
```

Dann in App-Settings auf `phi` umstellen.

---

## üìä Verf√ºgbare Models

| Model | Gr√∂√üe | Geschwindigkeit | Qualit√§t |
|-------|-------|-----------------|----------|
| **phi** | 1.5GB | ‚ö°‚ö°‚ö° Sehr schnell | ‚≠ê‚≠ê Gut |
| **llama3** | 4GB | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut |
| **mistral** | 4GB | ‚ö°‚ö° Schnell | ‚≠ê‚≠ê‚≠ê‚≠ê Sehr gut |
| **codellama** | 7GB | ‚ö° Mittel | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exzellent (Code) |
| **mixtral** | 26GB | üê¢ Langsam | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Top |

**Empfehlung f√ºr Trading-App:** `llama3` (bester Kompromiss)

---

## üöÄ Automatischer Start (optional)

Damit Ollama automatisch beim Mac-Start l√§uft:

```bash
# LaunchAgent erstellen
brew services start ollama
```

Dann l√§uft Ollama immer im Hintergrund! ‚úÖ

---

## üí° Vorteile von Ollama

‚úÖ **Kostenlos** - keine API-Kosten!
‚úÖ **Privat** - Daten verlassen Ihren Mac nicht
‚úÖ **Offline** - funktioniert ohne Internet
‚úÖ **Schnell** - keine API-Latenz
‚úÖ **Unbegrenzt** - keine Rate-Limits

---

## üîÑ Zwischen Cloud & Lokal wechseln

Sie k√∂nnen jederzeit zwischen verschiedenen KI-Providern wechseln:

1. **Ollama (Lokal)** - f√ºr Privatsph√§re & kostenlos
2. **Emergent LLM Key** - f√ºr beste Ergebnisse (GPT-5, Claude)
3. **Eigene API Keys** - OpenAI, Gemini, Claude

Einfach in den Einstellungen umstellen! üéØ

---

**Fragen?** Siehe `/app/electron/README.md` f√ºr mehr Details.
